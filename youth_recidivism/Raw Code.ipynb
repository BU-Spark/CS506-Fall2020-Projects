{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as py\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "def save_to_csv(file_name):\n",
    "\tdata = pd.io.stata.read_stata(file_name + '.dta')\n",
    "\tdata.to_csv(file_name + '.csv')\n",
    "\n",
    "def extract_label_and_feature_column(file_name):\n",
    "\tdf = pd.read_csv(file_name)\n",
    "\treturn df['WHO_WAS_CONTACTED'], df['COMMENTS_CLOB']\n",
    "\n",
    "def numerize_label(string):\n",
    "\tif string != string:\n",
    "\t\treturn 0\n",
    "\tif 'Client' in string:\n",
    "\t\treturn 1\n",
    "\tif 'Client' not in string and 'Collateral' in string:\n",
    "\t\treturn 3\n",
    "\tif 'Client' not in string:\n",
    "\t\treturn 2\n",
    "\t\t\n",
    "def process_label_and_feature(df,X):\n",
    "\tdf = df.apply(lambda x: numerize_label(x))\n",
    "\tna_values = get_na_values(X)\n",
    "\tdf = df.drop(na_values)\n",
    "\tX = X.drop(na_values)\n",
    "\treturn df,X\n",
    "\n",
    "def get_na_values(X):\n",
    "\treturn np.where(X.isna())[0].tolist()\n",
    "\n",
    "def clear(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    word_tokens=nltk.word_tokenize(x)\n",
    "    word_tokens=[lemmatizer.lemmatize(x) for x in word_tokens]\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]  \n",
    "    return filtered_sentence \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lw/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (1,44,46) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "Y, X = extract_label_and_feature_column('DYS_Casenotes_16Oct20_merged_final_bydischarge.csv')\n",
    "Y, X = process_label_and_feature(Y,X)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 430031),\n",
       " ('wa', 108291),\n",
       " (',', 99941),\n",
       " ('cw', 55223),\n",
       " (\"'s\", 46599),\n",
       " ('home', 46287),\n",
       " ('ha', 36586),\n",
       " ('school', 36398),\n",
       " ('mother', 33010),\n",
       " ('met', 32018),\n",
       " ('today', 30115),\n",
       " ('reported', 29784),\n",
       " ('spoke', 29714),\n",
       " ('would', 29447),\n",
       " ('program', 28153),\n",
       " ('client', 24462),\n",
       " ('well', 24311),\n",
       " ('meeting', 24072),\n",
       " ('caseworker', 23770),\n",
       " ('also', 23062),\n",
       " ('time', 22013),\n",
       " ('youth', 21665),\n",
       " ('called', 20815),\n",
       " ('stated', 20112),\n",
       " ('informed', 19116),\n",
       " ('work', 18860),\n",
       " ('report', 18748),\n",
       " ('going', 18548),\n",
       " ('week', 16915),\n",
       " ('back', 16906),\n",
       " ('get', 16842),\n",
       " ('call', 16114),\n",
       " ('office', 16093),\n",
       " ('writer', 16070),\n",
       " ('issue', 15817),\n",
       " ('go', 15740),\n",
       " ('went', 15544),\n",
       " ('asked', 15518),\n",
       " ('visit', 15162),\n",
       " (')', 14895),\n",
       " ('discussed', 14881),\n",
       " ('(', 14750),\n",
       " ('court', 14383),\n",
       " ('told', 14361),\n",
       " ('mom', 14255),\n",
       " ('day', 14178),\n",
       " ('house', 14130),\n",
       " ('said', 14038),\n",
       " ('know', 13702),\n",
       " ('dy', 12683),\n",
       " ('next', 12638),\n",
       " ('check', 12555),\n",
       " ('could', 11840),\n",
       " ('-', 11394),\n",
       " ('job', 11082),\n",
       " ('need', 10815),\n",
       " ('tomorrow', 10788),\n",
       " ('see', 10708),\n",
       " ('family', 10654),\n",
       " ('let', 10519),\n",
       " ('meet', 10357),\n",
       " ('phone', 10226),\n",
       " ('case', 10089),\n",
       " (':', 9975),\n",
       " (\"n't\", 9654),\n",
       " ('good', 9142),\n",
       " ('able', 9111),\n",
       " ('received', 8657),\n",
       " ('pas', 8501),\n",
       " ('plan', 8456),\n",
       " ('community', 8310),\n",
       " ('staff', 8196),\n",
       " ('clinician', 8142),\n",
       " ('want', 8045),\n",
       " ('new', 8034),\n",
       " ('date', 8022),\n",
       " ('father', 7683),\n",
       " ('contact', 7649),\n",
       " ('working', 7504),\n",
       " ('doe', 7497),\n",
       " ('scheduled', 7484),\n",
       " ('still', 7333),\n",
       " ('checked', 7119),\n",
       " ('weekend', 7117),\n",
       " ('due', 7090),\n",
       " ('last', 6864),\n",
       " ('foster', 6765),\n",
       " ('take', 6698),\n",
       " ('picked', 6625),\n",
       " ('transported', 6604),\n",
       " ('thing', 6569),\n",
       " ('set', 6532),\n",
       " ('like', 6516),\n",
       " ('rrt', 6488),\n",
       " ('behavior', 6394),\n",
       " ('sent', 6251),\n",
       " ('left', 6234),\n",
       " ('come', 6224),\n",
       " ('treatment', 6174),\n",
       " ('concern', 6100)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.apply(clear)\n",
    "from collections import Counter\n",
    "all_token=[]\n",
    "for x in X:\n",
    "    all_token+=x\n",
    "Counter(all_token).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "addlist=['.',',','cw','spoke','!','?']\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for x in addlist:\n",
    "    stop_words.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spoke', 'clerk', 'office', 'regarding', 'danny', 'going', 'physically', 'transported', 'court', 'today', 'sheriff_department', 'case', 'pm']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['regard', 'danny', 'go', 'physically', 'transport', 'court', 'today', 'case', 'pm']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en',disable=['parser', 'ner'])\n",
    "\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140148\n"
     ]
    }
   ],
   "source": [
    "print(len(data_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.054*\"program\" + 0.031*\"meeting\" + 0.029*\"client\" + 0.025*\"discuss\" + '\n",
      "  '0.023*\"case\" + 0.022*\"family\" + 0.019*\"next\" + 0.017*\"sign\" + '\n",
      "  '0.017*\"transport\" + 0.016*\"court\"'),\n",
      " (1,\n",
      "  '0.082*\"go\" + 0.061*\"report\" + 0.058*\"meet\" + 0.053*\"home\" + 0.045*\"today\" + '\n",
      "  '0.041*\"work\" + 0.033*\"well\" + 0.030*\"visit\" + 0.026*\"week\" + 0.026*\"youth\"'),\n",
      " (2,\n",
      "  '0.027*\"would\" + 0.027*\"call\" + 0.021*\"also\" + 0.021*\"school\" + '\n",
      "  '0.021*\"state\" + 0.021*\"mother\" + 0.017*\"ask\" + 0.016*\"inform\" + '\n",
      "  '0.016*\"tell\" + 0.015*\"say\"'),\n",
      " (3,\n",
      "  '0.049*\"weekend\" + 0.047*\"text\" + 0.043*\"charge\" + 0.035*\"incident\" + '\n",
      "  '0.032*\"approve\" + 0.028*\"involve\" + 0.026*\"curfew\" + 0.018*\"communicate\" + '\n",
      "  '0.016*\"card\" + 0.015*\"event\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "lda_model.save('lda.model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140148\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"test.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(corpus, fp)\n",
    "with open(\"test.txt\", \"rb\") as fp:   # Unpickling\n",
    "    b = pickle.load(fp)\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = lda_model.get_document_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.33527306), (1, 0.25884923), (2, 0.37620497), (3, 0.029672703)]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = []\n",
    "prob = []\n",
    "for i in predicted_labels:\n",
    "    label,probability = max(i, key=lambda x:x[1])\n",
    "    new_label.append(label)\n",
    "    prob.append(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = np.array(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_0 = np.where(new_label == 0)[0]\n",
    "new_label_1 = np.where(new_label == 1)[0]\n",
    "new_label_2 = np.where(new_label == 2)[0]\n",
    "new_label_3 = np.where(new_label == 3)[0]\n",
    "Y_0 = np.where(Y == 0)[0]\n",
    "Y_1 = np.where(Y == 1)[0]\n",
    "Y_2 = np.where(Y == 2)[0]\n",
    "Y_3 = np.where(Y == 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(3063,) (85109,) (21217,) (30759,)\n",
      "staffing, youth, no_youth, collateral\n",
      "(11199,) (15013,) (113915,) (21,)\n"
     ]
    }
   ],
   "source": [
    "print(max(Y))\n",
    "print(Y_0.shape,Y_1.shape,Y_2.shape,Y_3.shape)\n",
    "print(\"staffing, youth, no_youth, collateral\")\n",
    "print(new_label_0.shape,new_label_1.shape,new_label_2.shape,new_label_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64538\n",
      "28805\n"
     ]
    }
   ],
   "source": [
    "s = set(Y_1)\n",
    "s1 =set (new_label_2)\n",
    "print(len(s.intersection(s1)))  \n",
    "print(113914-85109)\n",
    "#portion of Y_1 belongs to new_label_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1088\n"
     ]
    }
   ],
   "source": [
    "s = set(Y_2)\n",
    "s1 = set(new_label_1)\n",
    "print(len(s.intersection(s1)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19371\n"
     ]
    }
   ],
   "source": [
    "s = set(Y_2)\n",
    "s1 = set(new_label_2)\n",
    "print(len(s.intersection(s1)))  \n",
    "# Most Y_2 belongs to new_label_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27745\n"
     ]
    }
   ],
   "source": [
    "s = set(Y_3)\n",
    "s1 = set(new_label_2)\n",
    "print(len(s.intersection(s1)))\n",
    "# Y_3 belongs to new_label_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12620\n"
     ]
    }
   ],
   "source": [
    "s = set(Y_1)\n",
    "s1 = set(new_label_1)\n",
    "print(len(s.intersection(s1)))  \n",
    "#The other portion of Y_1 belongs to new_label_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19371\n"
     ]
    }
   ],
   "source": [
    "s = set(Y_2)\n",
    "s1 = set(new_label_2)\n",
    "print(len(s.intersection(s1)))  \n",
    "# Most of Y_0 belongs to new_label_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2261\n"
     ]
    }
   ],
   "source": [
    "s = set(Y_0)\n",
    "s1 = set(new_label_2)\n",
    "print(len(s.intersection(s1)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7934\n"
     ]
    }
   ],
   "source": [
    "s = set(Y_1)\n",
    "s1 = set(new_label_0)\n",
    "print(len(s.intersection(s1)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion is that collateral, staffing notes, and only parent/guardian present\n",
    "# should be combined, there is no difference. \n",
    "# the Youth and also youth with other people present should be divided into 3 categories. \n",
    "# further analysis needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
